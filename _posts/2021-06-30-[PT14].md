---
layout: single
title:  "[PT14] Week14"
category: 'Probability Theory'
toc: true
toc_sticky: true
---


서울대학교 통계학과 대학원 2021년도 1학년 1학기 확률론1 수업의 정리내용입니다. <br/>
> **reference**: Lecture note (Prof.sangSangyeol Lee),<br/> Probability: Theory and Examples, Rick Durrett, Version 5 January 11, 2019



### $\textbf{Theorem}$ 
Let $\lbrace \mu_n : n \geq 1 \rbrace$ be a seq of prob measure ans let $\varphi_n(t) = \int e^{itx} d\mu_n(x)$.<br/> If $\varphi_n (t) \rightarrow \varphi(t), \forall t, n \rightarrow \infty$ and $\varphi$ is conti at $t=0$, $\Longrightarrow \varphi$ is Ch.f and $\exists$prob measure $\mu$ s.t. $\mu_n \xrightarrow{w} \mu$ and $\varphi(t) = \int e^{itx}d\mu(x)$
 

<br/><br/>

### $\textbf{Lemma}$ 
Let $\mu$ be a prob measure and $\varphi(t) = \int e^{itx} d\mu(x)$. Then

<center>

$\forall \varepsilon > 0, \mu[-\varepsilon, \varepsilon] \geq \dfrac{\varepsilon}{2} \vert \int^{2/\varepsilon}_{-2/\varepsilon} \varphi(t)dt \vert - 1 $

</center>

<br/><br/>


### $\textbf{Theorem}$ 
Assume $X,Y$ are indep and $X=Y$ with mean 0 and variance 1. If $X+Y, X-Y$ are indep, then $X,Y$ are normal r.v.s

<br/><br/>

## Central Limit Theorem
Let $S_n = X_1 + \cdots + X_n, X_i \sim iid ~~ (0, \sigma^2)$

<center>

$\dfrac{S_n}{\sqrt{n}\sigma} \xrightarrow{d} N(0,1) $ as $n \rightarrow \infty$

</center>

<br/>

### $\textbf{Definition[Lindeberg's condition of CLT]}$ 
Let $\lbrace X_{nk} : k=1, \cdots , r_n \rbrace, r_n \in \mathbb{Z}^+ $ and $r_n \nearrow \infty$ be a double array of r.v.s s.t. 
1. $X_{n1} , \cdots , X_{nr_n}$ indep
2. $EX_{nk} = 0 , \forall nk$
3. $EX^2_{nk} < \infty$

Then $\lbrace X_{nk} \rbrace $ satisfies Lindeberg's condition, if 

<center>

$\forall \varepsilon >0 , \lim_{n \rightarrow \infty} \dfrac{1}{S_n^2} \Sigma^{r_n}_{k=1} \int _{\vert X_{nk} \vert \geq \varepsilon \cdot S_n } X_{nk}^2 dP = 0$

</center>

$S_n^2 = \sigma_{n1}^2  + \cdots + \sigma_{nR_n}^2, \sigma_{nk}^2 = EX_{nk}^2$  

<br/><br/>

### $\textbf{Theorem}$ 
Set $S_n = X_{n1} + \cdots + X_{nr_n}$, Lindeberg's conditions.

<center>

$\dfrac{S_n}{s_n} \xrightarrow{d} N(0,1)$

</center>

$s_n^2 = \sigma_{n1}^2 + \cdots + \sigma_{nr_n}^2$

<br/><br/>

### $\textbf{Definition[Lyapounov's condition]}$ 
$\lbrace X_{nk} \rbrace $ stisfies Lyapounov's condition, if for some $\delta > 0$
1. $EX_{nk} = 0$
2. $E\vert X_{nk}\vert^{2+\delta} < \infty$
3. $\lim_{n \rightarrow \infty} \Sigma ^{r_n}_{k=1} \vert X_{nk} \vert ^{2+ \delta} / s_n^{2+\delta} = 0$

<br/>

   * Lyapounov's condi $\rightarrow$ Lindeberg's condi
   * Lyapounov's condi $\nleftarrow$ Lindeberg's condi



<br/><br/>

### $\textbf{Theorem[Felle's theorem]}$ 
Lindeberg's condition holds $\Leftrightarrow $ CLT holds and $\dfrac{\max_{1\leq k \leq r_n} \sigma_{nk}^2 }{s_n^2} \rightarrow 0$


<br/><br/>

### $\textbf{Theorem}$ 
For $n \geq 1 , X_{nm}, m=1, \cdots , n $ are indep r.v.s with $P(X_{nm}=1) = P_{nm} = 1 - P(X_{nm} = 0 )$. Assume 
1. $\Sigma^n_{m=1} P_{nm} \rightarrow \lambda \in (0,\infty)$
2. $\max_{1 \leq m \leq n} P_{nm} \rightarrow 0$

Then  $S_n  = X_{n1} + \cdots + X_{nn} \xrightarrow{d} Poisson(\lambda)$




<br/><br/>

### $\textbf{Theorem}$ 
Let $X_{nm}, m=1, \cdots , n $ be indep r.v.s taking values $0,1,2, \cdots $ with $P(X_{nm}=1) = P_{nm}$, $P(X_{nm}\geq 2) = \varepsilon _{nm}$ s.t. 
1. $\Sigma^n_{m=1} P_{nm} \rightarrow 0$
2. $\max_{1 \leq m \leq n} P_{nm} \rightarrow 0$

Then $S_n \xrightarrow{d} Poisson(\lambda)$